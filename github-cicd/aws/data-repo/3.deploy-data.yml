# This workflow will do a clean installation of node dependencies, cache/restore them, build the source code and run tests across different versions of node
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-nodejs-with-github-actions

name: 3.Data Deploy

on:
  workflow_dispatch:
  workflow_call:
    inputs:
      modifiedJsonFileNames:
        description: Modified json file names with double quoute and comma
        type: string
        required: true
      environment:
        description: Target Branch
        type: string
        required: true

jobs:
  # upload generated json to patch s3(CDN)
  upload:
    name: Upload json to patch S3
    runs-on: ubuntu-20.04
    steps:
      - name: Check out branch
        uses: actions/checkout@v2

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.TEX_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.TEX_AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Set short sha
        id: vars
        run: echo "::set-output name=short_sha::$(git rev-parse --short HEAD)"

      - name: Set secrets to env
        uses: noliran/branch-based-secrets@v1
        with:
          # This action makes env which concatenated with PATCH_S3_BUCKET and Branch name.
          # So, we can find branch based secret dynamic.
          # example _) PATCH_S3_BUCKET_DEVELOP
          secrets: PATCH_S3_BUCKET

      - name: Deploy to S3
        env:
          COMMIT_HASH: ${{ steps.vars.outputs.short_sha }}
          PATCH_S3_BUCKET: ${{ secrets[env.PATCH_S3_BUCKET_NAME] }}
        run: |
          aws s3 sync ./Convertor/ClientJson/ s3://${{ env.PATCH_S3_BUCKET }}/Data/client/latest --delete
          aws s3 sync ./Convertor/ClientJson/ s3://${{ env.PATCH_S3_BUCKET }}/Data/client/${{ env.COMMIT_HASH }} --delete

  # copy data files to ec2 using sftp
  rsync:
    name: Upload data files to ec2 for server
    runs-on: ubuntu-20.04
    steps:
      - name: Check out branch
        uses: actions/checkout@v2
        with:
          ref: ${{ github.event.workflow_run.head_branch }}

      - name: Set secrets
        uses: noliran/branch-based-secrets@v1
        with:
          secrets: BASTION_PEM,BASTION_DNS

      - name: Uplaod data files to Bastion Server
        uses: easingthemes/ssh-deploy@main
        env:
          SSH_PRIVATE_KEY: ${{ secrets[env.BASTION_PEM_NAME] }}
          REMOTE_HOST: ${{ secrets[env.BASTION_DNS_NAME] }}
          REMOTE_USER: ec2-user
          SOURCE: "Convertor/ServerJson/data/"
          TARGET: /home/ec2-user/${{ inputs.environment }}
          ARGS: "-avzr --delete"

      - name: Uplaod data reload api shell script to Bastion Server
        uses: easingthemes/ssh-deploy@main
        env:
          SSH_PRIVATE_KEY: ${{ secrets[env.BASTION_PEM_NAME] }}
          REMOTE_HOST: ${{ secrets[env.BASTION_DNS_NAME] }}
          REMOTE_USER: ec2-user
          SOURCE: "static_publish.sh"
          TARGET: "/home/ec2-user/"
          ARGS: "-avzr --delete"

  # curl update version and server memory reload api
  update_and_reload:
    needs:
      - upload
      - rsync
    name: Update version info and Call reload api
    runs-on: ubuntu-20.04
    steps:
      - name: Set secrets
        uses: noliran/branch-based-secrets@v1
        with:
          secrets: BASTION_PEM,BASTION_DNS,DATA_PUBLISH_API_DOMAIN

      - name: Curl reload api
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets[env.BASTION_DNS_NAME] }}
          port: 22
          username: ec2-user
          key: ${{ secrets[env.BASTION_PEM_NAME] }}
          script: |
            export DATA_PUBLISH_API_DOMAIN_NAME=${{ secrets[env.DATA_PUBLISH_API_DOMAIN_NAME] }}
            export DATA_PUBLISH_API_KEY=${{ secrets.DATA_PUBLISH_API_KEY }}
            sh ./static_publish.sh ${{ inputs.modifiedJsonFileNames }}
